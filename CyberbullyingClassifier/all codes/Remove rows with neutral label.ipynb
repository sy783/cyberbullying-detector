{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f3d7c02",
   "metadata": {},
   "source": [
    "## Cyberbullying Detection: A Machine Learning Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecf748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import pandas as pd \n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da2e369",
   "metadata": {},
   "source": [
    "### Step 1: Load the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d28f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cyberbullying_tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ce5fa",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea32a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to set the column width to maximum \n",
    "pd.set_option('display.max_colwidth',150)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ed89f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0866b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the cyberbullying_type column \n",
    "\n",
    "df.drop([\"cyberbullying_type\"], axis = 1, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30205cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing values in data\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeae2e3",
   "metadata": {},
   "source": [
    "### Step 2 : Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c3c141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to convert uppercase to lowercase characters\n",
    "def lower_word(t):\n",
    "    new_text = \"\".join(t.lower())\n",
    "    return new_text\n",
    "\n",
    "df['lowercased'] = df['tweet_text'].apply(lambda x: lower_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c79c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove usernames, url and non utf8/ascii characters \n",
    "def rem_url(t):\n",
    "    text1 = \"\".join(re.sub(r'(?:\\@|https?\\://)\\S+', '', t))\n",
    "    text = \"\".join(re.sub(r'[^\\x00-\\x7f]',r'', text1))\n",
    "    return text\n",
    "\n",
    "df['no_url_and_username'] = df['lowercased'].apply(lambda x: rem_url(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb6d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to remove punctuation \n",
    "def rem_punc(t):\n",
    "        new_text = \"\".join(re.sub(r'[^\\w\\s]', '', t))\n",
    "        return new_text\n",
    "\n",
    "df['no_punctuation'] = df['no_url_and_username'].apply(lambda x: rem_punc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac15911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to break the sentence into tokens\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def create_token(t):\n",
    "        token_text = \" \".join(word_tokenize(t))\n",
    "        return token_text\n",
    "    \n",
    "df['tokenized'] = df['no_punctuation'].apply(lambda x: create_token(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693edb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split strings into list and join as string \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.remove('not')\n",
    "stop_words.extend(['rt', 'mkr', 'httpâ', 'tvwâ', 'etc'])\n",
    "\n",
    "def rem_stopword(t):\n",
    "    new_text = \" \".join([word for word in t.split() if word not in stop_words])\n",
    "    return new_text\n",
    "\n",
    "df['no_stopwords'] = df['tokenized'].apply(lambda x: rem_stopword(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad0d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total number of rows after datapreprocessing\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec6e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check for duplicated cells after cleaning\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ba4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicated cells\n",
    "df.drop_duplicates(\"no_stopwords\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432cf0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4dfbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORDNET LEMMATIZER (with appropriate pos tags)\n",
    "from nltk.stem import wordnet \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def lemma_postag(t):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define function to lemmatize each word with its POS tag\n",
    "\n",
    "    def pos_tagger(nltk_tag):\n",
    "        if nltk_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif nltk_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif nltk_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif nltk_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # tokenize the sentence and find the POS tag for each token\n",
    "    pos_tagged = nltk.pos_tag(nltk.word_tokenize(t))\n",
    "\n",
    "    # our own pos_tagger function to make things simpler to understand.\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            # if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:\n",
    "            # else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    "\n",
    "    return lemmatized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0212da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization_pos_tag is chosen after analysis\n",
    "\n",
    "df['lemmatization_postag'] = df['no_stopwords'].apply(lambda x: lemma_postag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0623ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7cfac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since data is cleaned, so all columns are dropped except for lemmatization_postag\n",
    "\n",
    "clean_data = df.drop(['lowercased', 'no_url_and_username', 'no_punctuation', 'no_stopwords', 'tokenized'], axis = 1)\n",
    "clean_data.rename(columns = {\"lemmatization_postag\":\"cleaned_tweet\"}, inplace = True)\n",
    "clean_data.rename(columns = {\"tweet_text\":\"original_tweet\"}, inplace = True)\n",
    "\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379b41b2",
   "metadata": {},
   "source": [
    "### Step 3: Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e302276",
   "metadata": {},
   "source": [
    "#### To label the data into their respective categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfff7cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "#TextBlob\n",
    "def getPolarity_TB(t):\n",
    "    result = TextBlob(t).sentiment.polarity\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce360e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_blob = clean_data.copy(deep=True)\n",
    "\n",
    "text_blob['TextBlob_polarity_originalTweet'] = text_blob['original_tweet'].apply(getPolarity_TB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c9a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_blob['TextBlob_polarity_cleanedTweet'] = text_blob['cleaned_tweet'].apply(getPolarity_TB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb84ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabel(polarity):\n",
    "    if polarity < 0:\n",
    "        return 'Negative'\n",
    "    elif polarity == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9575f69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_blob['TextBlob_label_originalTweet'] = text_blob['TextBlob_polarity_originalTweet'].apply(getLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e733fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_blob['TextBlob_label_cleanedTweet'] = text_blob['TextBlob_polarity_cleanedTweet'].apply(getLabel)\n",
    "text_blob.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f6633b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#to visualize the textblob\n",
    "text_blob.TextBlob_label_originalTweet.value_counts().plot(kind='bar',title=\"Sentiment Analysis for Original Tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35821d6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#to visualize the textblob\n",
    "text_blob.TextBlob_label_cleanedTweet.value_counts().plot(kind='bar',title=\"Sentiment Analysis for Cleaned Tweet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee173552",
   "metadata": {},
   "source": [
    "#### Remove tweets that is labelled as 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98de8610",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_blob.drop(text_blob[text_blob['TextBlob_label_cleanedTweet'] == \"Neutral\"].index, inplace=True)\n",
    "text_blob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e1773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove it since it will be done in the next line\n",
    "# text_blob.TextBlob_label_cleanedTweet.value_counts().plot(kind='bar',title=\"Sentiment Analysis for Cleaned Tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d41061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(text_blob['TextBlob_label_cleanedTweet'] == \"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0030b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the negative label to 1 , positive label to 0 \n",
    "# but first need to drop the rows, just remain the (cleaned_tweet, TextBlob_label_cleanedTweet, TextBlob_polarity_cleanedTweet)\n",
    "# need to add a new column (target)\n",
    "cleaned_table = text_blob.drop(['original_tweet','TextBlob_polarity_originalTweet', 'TextBlob_label_originalTweet'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3020a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_table.rename(columns = {'TextBlob_polarity_cleanedTweet':'polarity',\n",
    "                              'TextBlob_label_cleanedTweet':'label'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71463076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive - 0 for non-cyberbullying \n",
    "# Negative - 1 for cyberbullying\n",
    "\n",
    "cleaned_table['target'] = cleaned_table['label'].apply(lambda label : 1 if label == \"Negative\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c769984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "\n",
    "x=cleaned_table['target'].value_counts()\n",
    "sns.barplot(x.index,x).set(title=\"Updated Table with Negative and Positive Tweets\")\n",
    "\n",
    "sum = x[0] + x[1]\n",
    "percentage_positive = (x[0]/sum)*100\n",
    "percentage_negative = (x[1]/sum)*100\n",
    "diff = percentage_negative - percentage_positive\n",
    "\n",
    "print(\"Percentage of positive class (0) : {:.2f}%\".format(percentage_positive))\n",
    "print(\"Percentage of negative class (1) : {:.2f}%\".format(percentage_negative))\n",
    "print(\"Difference between two classes : {:.2f}%\".format(diff))\n",
    "\n",
    "\n",
    "# since the difference is not too large, so it is a quite balanced class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b19fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv to rename the label after dropping rows and columns \n",
    "# then need to rearrange the index number manually in the excel file\n",
    "cleaned_table.to_csv('cleaned_table.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
